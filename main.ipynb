{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import librosa\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNetLoader(Dataset):\n",
    "    def __init__(self, in_path, out_path, normalize = False, train = False, val = False, test = False):\n",
    "        self.input = in_path\n",
    "        self.output = out_path \n",
    "        self.train, self.val, self.test = train, val, test\n",
    "        \n",
    "        in_rate, in_data = wavfile.read(self.input)\n",
    "        out_rate, out_data = wavfile.read(self.output)\n",
    "        assert in_rate == out_rate, \"in_file and out_file must have same sample rate\"\n",
    "\n",
    "\n",
    "        # Load audio files  \n",
    "        def normalize(data):\n",
    "            data_max = max(data)\n",
    "            data_min = min(data)\n",
    "            data_norm = max(data_max,abs(data_min))\n",
    "            return data / data_norm\n",
    "        \n",
    "        # Trim the length of audio to equal the smaller wav file\n",
    "        if len(in_data) > len(out_data):\n",
    "            print(\"Trimming input audio to match output audio\")\n",
    "        in_data = in_data[0:len(out_data)]\n",
    "        if len(out_data) > len(in_data): \n",
    "            print(\"Trimming output audio to match input audio\")\n",
    "        out_data = out_data[0:len(in_data)]\n",
    "\n",
    "        # If stereo data, use channel 0\n",
    "        if len(in_data.shape) > 1:\n",
    "            print(\"[WARNING] Stereo data detected for in_data, only using first channel (left channel)\")\n",
    "            in_data = in_data[:,0]\n",
    "        if len(out_data.shape) > 1:\n",
    "            print(\"[WARNING] Stereo data detected for out_data, only using first channel (left channel)\")\n",
    "            out_data = out_data[:,0]\n",
    "\n",
    "        #normalize data\n",
    "        if normalize == True:\n",
    "            in_data = normalize(in_data)\n",
    "            out_data = normalize(out_data)\n",
    "\n",
    "        # Convert PCM16 to FP32\n",
    "        if in_data.dtype == \"int16\":\n",
    "            in_data = in_data/32767\n",
    "            print(\"In data converted from PCM16 to FP32\")\n",
    "        if out_data.dtype == \"int16\":\n",
    "            out_data = out_data/32767\n",
    "            print(\"Out data converted from PCM16 to FP32\")\n",
    "\n",
    "        self.sample_time = 100e-3\n",
    "        sample_size = int(in_rate * self.sample_time)\n",
    "        length = len(in_data) - len(in_data) % sample_size\n",
    "\n",
    "        x = in_data[:length].reshape((-1, 1, sample_size)).astype(np.float32)\n",
    "        y = out_data[:length].reshape((-1, 1, sample_size)).astype(np.float32)\n",
    "\n",
    "        split = lambda d: np.split(d, [int(len(d) * 0.6), int(len(d) * 0.8)])\n",
    "\n",
    "        self.d = {}\n",
    "        self.d[\"x_train\"], self.d[\"x_valid\"], self.d[\"x_test\"] = split(x)\n",
    "        self.d[\"y_train\"], self.d[\"y_valid\"], self.d[\"y_test\"] = split(y)\n",
    "        self.d[\"mean\"], self.d[\"std\"] = self.d[\"x_train\"].mean(), self.d[\"x_train\"].std()\n",
    "        for key in \"x_train\", \"x_valid\", \"x_test\":\n",
    "            self.d[key] = (self.d[key] - self.d[\"mean\"]) / self.d[\"std\"]\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        if self.val: \n",
    "            return len(self.d[\"x_valid\"])\n",
    "        \n",
    "        elif self.test: \n",
    "            return len(self.d[\"x_test\"])\n",
    "        \n",
    "        elif self.train: \n",
    "            return len(self.d[\"x_train\"])\n",
    "             \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if self.val: \n",
    "            return torch.from_numpy(self.d[\"x_valid\"]), torch.from_numpy(self.d[\"y_valid\"])\n",
    "        \n",
    "        elif self.test: \n",
    "            return torch.from_numpy(self.d[\"x_test\"]), torch.from_numpy(self.d[\"y_test\"])\n",
    "        \n",
    "        elif self.train: \n",
    "            return torch.from_numpy(self.d[\"x_train\"]), torch.from_numpy(self.d[\"y_train\"])\n",
    "\n",
    "\n",
    "in_path = \"D:\\\\Documents\\\\CMU_SUBJECTS\\\\BlackBoxAudioFx\\\\PedalNetRT\\\\data\\\\ts9_test1_in_FP32.wav\"\n",
    "out_path = \"D:\\\\Documents\\\\CMU_SUBJECTS\\\\BlackBoxAudioFx\\\\PedalNetRT\\\\data\\\\ts9_test1_out_FP32.wav\"  \n",
    "train_data = WaveNetLoader(in_path, out_path, normalize = True, train = True)\n",
    "test_data = WaveNetLoader(in_path, out_path, normalize = True, test = True)\n",
    "val_data = WaveNetLoader(in_path, out_path, normalize = True, val = True)\n",
    "\n",
    "num_workers, batch_size = 4, 64\n",
    "train_loader = DataLoader(train_data, batch_size = 64, num_workers= num_workers)\n",
    "val_loader = DataLoader(val_data, batch_size = 64, num_workers= num_workers)\n",
    "test_loader = DataLoader(test_data, batch_size = 64, num_workers= num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Error function of choice is the error to signal ratio \n",
    "\"\"\"\n",
    "def pre_emp(x, coeff = 0.95):\n",
    "\n",
    " \"\"\" y[n] = x[n] - coeff * x[n-1] \n",
    "    coefficient adapted from paper : 0.95\n",
    " \"\"\" \n",
    " torch.cat(x[:,:,0:1], x[:,:,1:] - coeff * x[:,:,:-1], dim = 2)\n",
    "\n",
    "def error_to_signal(y,y_pred): \n",
    "   y,y_pred = pre_emp(y), pre_emp(y_pred, )\n",
    "   return np.sum(np.power(y-y_pred,2), dim = 2 ) / (np.sum(np.power(y,2), dim = 2)  +  1e-10)\n",
    "\n",
    "\n",
    "# sample_data = \"D:\\\\Documents\\\\CMU_SUBJECTS\\\\BlackBoxAudioFx\\\\NeuralAudioModelling\\\\ts9_test1_in_FP32.wav\"\n",
    "# audio,_ = librosa.load(sample_data)\n",
    "\n",
    "# audio_copy = audio.copy()\n",
    "# print(error_to_signal(audio,audio_copy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dilated causal convolutions in WaveNet\n",
    "\n",
    "Causal convolutions \n",
    "Causal comes from causality, which means if we have a canonical 'direction' we are reading our data, then data that is ahead of the current position cannot factor \n",
    "into the calculation. This is most obvious in time series, so only previous timesteps factor into the current and not something 'future' relative to the current. \n",
    "But note it can also be applied to other forms of data like 2D images (like in PixelCNN for e.g.)\n",
    "\n",
    "The causal convolution concept comes about because when you do convolution, the kernel may overlap with the data from the 'future' points thus breaking causality. \n",
    "We don't want this so usually we introduce some kind of zero masking onto these points. This masking procedure is what sets apart causal convolution from standard \n",
    "convolution.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class CausalConv1d(torch.nn.Conv1d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n",
    "        self.__padding = (kernel_size - 1) * dilation\n",
    "\n",
    "        super(CausalConv1d, self).__init__(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding = self.__padding,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        result = super(CausalConv1d, self).forward(input)\n",
    "        if self.__padding != 0:\n",
    "            return result[:, :, : -self.__padding]\n",
    "        return result\n",
    "\n",
    "def _conv_stack(dilations, in_channels, out_channels, kernel_size):\n",
    "    \"\"\"\n",
    "    Create stack of dilated convolutional layers, outlined in WaveNet paper:\n",
    "    https://arxiv.org/pdf/1609.03499.pdf\n",
    "    \"\"\"\n",
    "    return nn.ModuleList(\n",
    "        [\n",
    "            CausalConv1d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                dilation=d,\n",
    "                kernel_size=kernel_size,\n",
    "            )\n",
    "            for i, d in enumerate(dilations)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, num_channels, dilation_depth, num_repeat, kernel_size = 2 ): \n",
    "        super(WaveNet,self).__init__()\n",
    "        dilation = [2* d for d in range(dilation_depth)] * num_repeat\n",
    "        internal_channels = int(num_channels * 2)\n",
    "        self.hidden = _conv_stack(dilation, num_channels, internal_channels, kernel_size)\n",
    "        self.residuals = _conv_stack(dilation, num_channels, num_channels, 1)\n",
    "        self.input_layer = CausalConv1d(\n",
    "            in_channels=1,\n",
    "            out_channels=num_channels,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "\n",
    "        self.linear_mix = nn.Conv1d(\n",
    "            in_channels=num_channels * dilation_depth * num_repeat,\n",
    "            out_channels=1,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self,x): \n",
    "        out = x\n",
    "        skips = []\n",
    "        out = self.input_layer(out)\n",
    "\n",
    "        for hidden, residual in zip(self.hidden, self.residuals):\n",
    "            x = out\n",
    "            out_hidden = hidden(x)\n",
    "\n",
    "            # gated activation\n",
    "            # split (32,16,3) into two (16,16,3) for tanh and sigm calculations\n",
    "            out_hidden_split = torch.split(out_hidden, self.num_channels, dim=1)\n",
    "            out = torch.tanh(out_hidden_split[0]) * torch.sigmoid(out_hidden_split[1])\n",
    "\n",
    "            skips.append(out)\n",
    "\n",
    "            out = residual(out)\n",
    "            out = out + x[:, :, -out.size(2) :]\n",
    "\n",
    "        # modified \"postprocess\" step:\n",
    "        out = torch.cat([s[:, :, -out.size(2) :] for s in skips], dim=1)\n",
    "        out = self.linear_mix(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─ModuleList: 1-1                        --\n",
      "|    └─CausalConv1d: 2-1                 104\n",
      "|    └─CausalConv1d: 2-2                 104\n",
      "|    └─CausalConv1d: 2-3                 104\n",
      "|    └─CausalConv1d: 2-4                 104\n",
      "|    └─CausalConv1d: 2-5                 104\n",
      "|    └─CausalConv1d: 2-6                 104\n",
      "|    └─CausalConv1d: 2-7                 104\n",
      "|    └─CausalConv1d: 2-8                 104\n",
      "|    └─CausalConv1d: 2-9                 104\n",
      "|    └─CausalConv1d: 2-10                104\n",
      "|    └─CausalConv1d: 2-11                104\n",
      "|    └─CausalConv1d: 2-12                104\n",
      "|    └─CausalConv1d: 2-13                104\n",
      "|    └─CausalConv1d: 2-14                104\n",
      "|    └─CausalConv1d: 2-15                104\n",
      "|    └─CausalConv1d: 2-16                104\n",
      "|    └─CausalConv1d: 2-17                104\n",
      "|    └─CausalConv1d: 2-18                104\n",
      "├─ModuleList: 1-2                        --\n",
      "|    └─CausalConv1d: 2-19                20\n",
      "|    └─CausalConv1d: 2-20                20\n",
      "|    └─CausalConv1d: 2-21                20\n",
      "|    └─CausalConv1d: 2-22                20\n",
      "|    └─CausalConv1d: 2-23                20\n",
      "|    └─CausalConv1d: 2-24                20\n",
      "|    └─CausalConv1d: 2-25                20\n",
      "|    └─CausalConv1d: 2-26                20\n",
      "|    └─CausalConv1d: 2-27                20\n",
      "|    └─CausalConv1d: 2-28                20\n",
      "|    └─CausalConv1d: 2-29                20\n",
      "|    └─CausalConv1d: 2-30                20\n",
      "|    └─CausalConv1d: 2-31                20\n",
      "|    └─CausalConv1d: 2-32                20\n",
      "|    └─CausalConv1d: 2-33                20\n",
      "|    └─CausalConv1d: 2-34                20\n",
      "|    └─CausalConv1d: 2-35                20\n",
      "|    └─CausalConv1d: 2-36                20\n",
      "├─CausalConv1d: 1-3                      8\n",
      "├─Conv1d: 1-4                            73\n",
      "=================================================================\n",
      "Total params: 2,313\n",
      "Trainable params: 2,313\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "hyperparameters \n",
    "\"\"\"\n",
    "\n",
    "num_channels = 4\n",
    "dilation_depth = 9\n",
    "num_repeat = 2 \n",
    "kernel_size =3\n",
    "learning_rate, batch_size = 3e-3, 64\n",
    "wavenet_model = WaveNet(\n",
    "            num_channels,\n",
    "            dilation_depth,\n",
    "            num_repeat,\n",
    "            kernel_size\n",
    "        )\n",
    "summary(wavenet_model) \n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device == \"cuda\"\n",
    "    wavenet_model = wavenet_model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(wavenet_model.parameters(), lr= learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wavenet_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m wavenet_model\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wavenet_model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = hparams[\"learning_rate\"]\n",
    "batch_size = hparams[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PointNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be71f7ebca6fc406e8f2b7eb97fc71d65ed284e191505e45141d8f3008802363"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
